# ğŸ’¾ FASE 1: BACKUP AUTOMÃTICO - PLANO DETALHADO

**Criticidade:** ğŸ”´ CRÃTICA  
**Timeline:** Semanas 3-4 (30 horas)  
**Status:** ğŸ“‹ Planejamento  
**DependÃªncia:** JWT Auth (Fase 1.1)  

---

## ğŸ“‹ VISÃƒO GERAL

Implementar sistema robusto de backup automÃ¡tico com recuperaÃ§Ã£o em minutos, armazenamento redundante e conformidade com SLA.

---

## ğŸ¯ OBJETIVOS

1. âœ… Backup automÃ¡tico diÃ¡rio
2. âœ… MÃºltiplas cÃ³pias (local + cloud)
3. âœ… RecuperaÃ§Ã£o em < 1 hora
4. âœ… RetenÃ§Ã£o de 30 dias
5. âœ… Testes de recuperaÃ§Ã£o automatizados
6. âœ… Alertas de falha

---

## ğŸ—ï¸ ARQUITETURA DE BACKUP

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BANCO DE DADOS (PostgreSQL)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dados em produÃ§Ã£o                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BACKUP SERVICE                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚ 1. TRIGGER DIÃRIO (00:00)                              â”‚
â”‚    â””â”€ Inicia backup automÃ¡tico                         â”‚
â”‚                                                         â”‚
â”‚ 2. DUMP DO BANCO                                        â”‚
â”‚    â”œâ”€ pg_dump (PostgreSQL)                             â”‚
â”‚    â”œâ”€ CompressÃ£o gzip                                  â”‚
â”‚    â””â”€ Arquivo: backup-YYYY-MM-DD.sql.gz               â”‚
â”‚                                                         â”‚
â”‚ 3. BACKUP DE UPLOADS                                    â”‚
â”‚    â”œâ”€ Sincroniza pasta /uploads                        â”‚
â”‚    â”œâ”€ CompressÃ£o tar.gz                                â”‚
â”‚    â””â”€ Arquivo: uploads-YYYY-MM-DD.tar.gz              â”‚
â”‚                                                         â”‚
â”‚ 4. ARMAZENAMENTO MÃšLTIPLO                               â”‚
â”‚    â”œâ”€ Local: /backups/local/                           â”‚
â”‚    â”œâ”€ S3: AWS S3 (redundÃ¢ncia)                         â”‚
â”‚    â””â”€ GCS: Google Cloud Storage (backup)               â”‚
â”‚                                                         â”‚
â”‚ 5. VERIFICAÃ‡ÃƒO                                          â”‚
â”‚    â”œâ”€ Valida integridade do arquivo                    â”‚
â”‚    â”œâ”€ Testa recuperaÃ§Ã£o (weekly)                       â”‚
â”‚    â””â”€ Envia relatÃ³rio                                  â”‚
â”‚                                                         â”‚
â”‚ 6. LIMPEZA                                              â”‚
â”‚    â”œâ”€ Remove backups > 30 dias                         â”‚
â”‚    â””â”€ Libera espaÃ§o em disco                           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š ESTRATÃ‰GIA DE BACKUP

### RetenÃ§Ã£o
```
DiÃ¡rio:   Ãšltimos 7 dias (local + S3)
Semanal:  Ãšltimas 4 semanas (S3)
Mensal:   Ãšltimos 12 meses (GCS)
Anual:    Indefinido (Arquivo)

EspaÃ§o estimado:
- Local: 50GB (7 dias)
- S3: 200GB (30 dias)
- GCS: 500GB (12 meses)
```

### RTO/RPO
```
RTO (Recovery Time Objective): < 1 hora
RPO (Recovery Point Objective): < 1 dia

Significa:
- RecuperaÃ§Ã£o em atÃ© 1 hora
- MÃ¡ximo 1 dia de dados perdidos
```

---

## ğŸ—‚ï¸ ESTRUTURA DE ARQUIVOS

```
server/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ backup.ts              # Script de backup
â”‚   â”œâ”€â”€ restore.ts             # Script de restauraÃ§Ã£o
â”‚   â””â”€â”€ verify-backup.ts       # Script de verificaÃ§Ã£o
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ backup.config.ts       # ConfiguraÃ§Ã£o
â”‚
â”œâ”€â”€ backups/
â”‚   â”œâ”€â”€ local/                 # Backups locais
â”‚   â”‚   â”œâ”€â”€ backup-2025-01-01.sql.gz
â”‚   â”‚   â”œâ”€â”€ uploads-2025-01-01.tar.gz
â”‚   â”‚   â””â”€â”€ backup-2025-01-02.sql.gz
â”‚   â”‚
â”‚   â””â”€â”€ logs/                  # Logs de backup
â”‚       â”œâ”€â”€ backup-2025-01-01.log
â”‚       â””â”€â”€ backup-2025-01-02.log
â”‚
â””â”€â”€ .env
    â”œâ”€â”€ BACKUP_SCHEDULE=0 0 * * *  # Cron: 00:00 diariamente
    â”œâ”€â”€ AWS_S3_BUCKET=conexa-backups
    â”œâ”€â”€ GCS_BUCKET=conexa-backups-archive
    â””â”€â”€ BACKUP_RETENTION_DAYS=30
```

---

## ğŸ”Œ ENDPOINTS DE BACKUP

```
GET    /api/admin/backups              # Listar backups
GET    /api/admin/backups/:id/download # Baixar backup
POST   /api/admin/backups/trigger      # ForÃ§ar backup agora
POST   /api/admin/backups/:id/restore  # Restaurar backup
GET    /api/admin/backups/status       # Status do Ãºltimo backup
DELETE /api/admin/backups/:id          # Deletar backup
```

---

## ğŸ“ IMPLEMENTAÃ‡ÃƒO

### 1. ConfiguraÃ§Ã£o (backup.config.ts)
```typescript
// server/config/backup.config.ts

export const backupConfig = {
  // Schedule
  schedule: process.env.BACKUP_SCHEDULE || '0 0 * * *', // 00:00 diariamente
  
  // RetenÃ§Ã£o
  retentionDays: parseInt(process.env.BACKUP_RETENTION_DAYS || '30'),
  
  // Armazenamento Local
  localPath: path.join(process.cwd(), 'backups', 'local'),
  logsPath: path.join(process.cwd(), 'backups', 'logs'),
  
  // AWS S3
  aws: {
    enabled: !!process.env.AWS_S3_BUCKET,
    bucket: process.env.AWS_S3_BUCKET,
    region: process.env.AWS_REGION || 'us-east-1',
    accessKeyId: process.env.AWS_ACCESS_KEY_ID,
    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY
  },
  
  // Google Cloud Storage
  gcs: {
    enabled: !!process.env.GCS_BUCKET,
    bucket: process.env.GCS_BUCKET,
    projectId: process.env.GCS_PROJECT_ID,
    keyFile: process.env.GCS_KEY_FILE
  },
  
  // NotificaÃ§Ãµes
  notifications: {
    email: process.env.BACKUP_NOTIFY_EMAIL,
    slack: process.env.SLACK_WEBHOOK_URL
  }
};
```

### 2. Script de Backup (backup.ts)
```typescript
// server/scripts/backup.ts

import { exec } from 'child_process';
import { promises as fs } from 'fs';
import path from 'path';
import { S3Client, PutObjectCommand } from '@aws-sdk/client-s3';
import { Storage } from '@google-cloud/storage';

export async function performBackup() {
  const timestamp = new Date().toISOString().split('T')[0]; // YYYY-MM-DD
  const backupDir = backupConfig.localPath;
  
  try {
    // 1. Criar diretÃ³rio se nÃ£o existir
    await fs.mkdir(backupDir, { recursive: true });
    
    // 2. Backup do banco de dados
    const dbBackupFile = path.join(backupDir, `backup-${timestamp}.sql.gz`);
    await backupDatabase(dbBackupFile);
    console.log(`âœ… Backup do banco criado: ${dbBackupFile}`);
    
    // 3. Backup de uploads
    const uploadsBackupFile = path.join(backupDir, `uploads-${timestamp}.tar.gz`);
    await backupUploads(uploadsBackupFile);
    console.log(`âœ… Backup de uploads criado: ${uploadsBackupFile}`);
    
    // 4. Upload para S3
    if (backupConfig.aws.enabled) {
      await uploadToS3(dbBackupFile, `backup-${timestamp}.sql.gz`);
      await uploadToS3(uploadsBackupFile, `uploads-${timestamp}.tar.gz`);
      console.log(`âœ… Backups enviados para S3`);
    }
    
    // 5. Upload para GCS
    if (backupConfig.gcs.enabled) {
      await uploadToGCS(dbBackupFile, `backup-${timestamp}.sql.gz`);
      await uploadToGCS(uploadsBackupFile, `uploads-${timestamp}.tar.gz`);
      console.log(`âœ… Backups enviados para GCS`);
    }
    
    // 6. Limpeza de backups antigos
    await cleanOldBackups();
    console.log(`âœ… Backups antigos removidos`);
    
    // 7. Enviar notificaÃ§Ã£o
    await notifySuccess(timestamp);
    
    return { success: true, timestamp };
  } catch (error) {
    console.error('âŒ Erro no backup:', error);
    await notifyFailure(error);
    throw error;
  }
}

// Backup do banco de dados
async function backupDatabase(outputFile: string) {
  return new Promise((resolve, reject) => {
    const command = `pg_dump ${process.env.DATABASE_URL} | gzip > ${outputFile}`;
    
    exec(command, (error) => {
      if (error) reject(error);
      else resolve(true);
    });
  });
}

// Backup de uploads
async function backupUploads(outputFile: string) {
  return new Promise((resolve, reject) => {
    const uploadsPath = path.join(process.cwd(), 'uploads');
    const command = `tar -czf ${outputFile} -C ${uploadsPath} .`;
    
    exec(command, (error) => {
      if (error) reject(error);
      else resolve(true);
    });
  });
}

// Upload para S3
async function uploadToS3(filePath: string, fileName: string) {
  const s3Client = new S3Client({
    region: backupConfig.aws.region,
    credentials: {
      accessKeyId: backupConfig.aws.accessKeyId!,
      secretAccessKey: backupConfig.aws.secretAccessKey!
    }
  });
  
  const fileContent = await fs.readFile(filePath);
  
  const command = new PutObjectCommand({
    Bucket: backupConfig.aws.bucket,
    Key: `backups/${fileName}`,
    Body: fileContent,
    ContentType: 'application/gzip',
    Metadata: {
      'backup-date': new Date().toISOString()
    }
  });
  
  await s3Client.send(command);
}

// Upload para GCS
async function uploadToGCS(filePath: string, fileName: string) {
  const storage = new Storage({
    projectId: backupConfig.gcs.projectId,
    keyFilename: backupConfig.gcs.keyFile
  });
  
  const bucket = storage.bucket(backupConfig.gcs.bucket);
  await bucket.upload(filePath, {
    destination: `backups/${fileName}`,
    metadata: {
      metadata: {
        'backup-date': new Date().toISOString()
      }
    }
  });
}

// Limpeza de backups antigos
async function cleanOldBackups() {
  const cutoffDate = new Date();
  cutoffDate.setDate(cutoffDate.getDate() - backupConfig.retentionDays);
  
  const files = await fs.readdir(backupConfig.localPath);
  
  for (const file of files) {
    const filePath = path.join(backupConfig.localPath, file);
    const stats = await fs.stat(filePath);
    
    if (stats.mtime < cutoffDate) {
      await fs.unlink(filePath);
      console.log(`ğŸ—‘ï¸  Deletado: ${file}`);
    }
  }
}

// NotificaÃ§Ãµes
async function notifySuccess(timestamp: string) {
  // Email
  if (backupConfig.notifications.email) {
    // Enviar email de sucesso
  }
  
  // Slack
  if (backupConfig.notifications.slack) {
    // Enviar mensagem Slack
  }
}

async function notifyFailure(error: any) {
  // Email
  if (backupConfig.notifications.email) {
    // Enviar email de erro
  }
  
  // Slack
  if (backupConfig.notifications.slack) {
    // Enviar alerta Slack
  }
}
```

### 3. Script de RestauraÃ§Ã£o (restore.ts)
```typescript
// server/scripts/restore.ts

export async function restoreBackup(backupFile: string) {
  try {
    console.log(`ğŸ”„ Iniciando restauraÃ§Ã£o de ${backupFile}...`);
    
    // 1. Validar arquivo
    const exists = await fileExists(backupFile);
    if (!exists) {
      throw new Error(`Arquivo nÃ£o encontrado: ${backupFile}`);
    }
    
    // 2. Criar backup de seguranÃ§a
    const safetyBackup = await performBackup();
    console.log(`âœ… Backup de seguranÃ§a criado: ${safetyBackup}`);
    
    // 3. Restaurar banco de dados
    await restoreDatabase(backupFile);
    console.log(`âœ… Banco de dados restaurado`);
    
    // 4. Restaurar uploads
    const uploadsFile = backupFile.replace('backup-', 'uploads-');
    if (await fileExists(uploadsFile)) {
      await restoreUploads(uploadsFile);
      console.log(`âœ… Uploads restaurados`);
    }
    
    // 5. Verificar integridade
    await verifyRestoration();
    console.log(`âœ… Integridade verificada`);
    
    return { success: true, backupFile };
  } catch (error) {
    console.error('âŒ Erro na restauraÃ§Ã£o:', error);
    throw error;
  }
}

async function restoreDatabase(backupFile: string) {
  return new Promise((resolve, reject) => {
    const command = `gunzip -c ${backupFile} | psql ${process.env.DATABASE_URL}`;
    
    exec(command, (error) => {
      if (error) reject(error);
      else resolve(true);
    });
  });
}

async function restoreUploads(backupFile: string) {
  return new Promise((resolve, reject) => {
    const uploadsPath = path.join(process.cwd(), 'uploads');
    const command = `tar -xzf ${backupFile} -C ${uploadsPath}`;
    
    exec(command, (error) => {
      if (error) reject(error);
      else resolve(true);
    });
  });
}

async function verifyRestoration() {
  // Conectar ao banco e verificar dados
  const count = await prisma.student.count();
  if (count === 0) {
    throw new Error('RestauraÃ§Ã£o falhou: nenhum aluno encontrado');
  }
}
```

### 4. Agendamento (cron job)
```typescript
// server/src/index.ts

import cron from 'node-cron';
import { performBackup } from '../scripts/backup';

// Agendar backup diÃ¡rio Ã s 00:00
cron.schedule('0 0 * * *', async () => {
  console.log('ğŸ”„ Iniciando backup automÃ¡tico...');
  try {
    await performBackup();
    console.log('âœ… Backup concluÃ­do com sucesso');
  } catch (error) {
    console.error('âŒ Erro no backup:', error);
  }
});

// Agendar verificaÃ§Ã£o semanal (domingo Ã s 02:00)
cron.schedule('0 2 * * 0', async () => {
  console.log('ğŸ” Iniciando verificaÃ§Ã£o de backup...');
  try {
    await verifyBackup();
    console.log('âœ… VerificaÃ§Ã£o concluÃ­da');
  } catch (error) {
    console.error('âŒ Erro na verificaÃ§Ã£o:', error);
  }
});
```

---

## ğŸ“Š ENDPOINTS DE BACKUP

### Listar Backups
```bash
GET /api/admin/backups
Authorization: Bearer {token}

Resposta:
{
  "backups": [
    {
      "id": "backup-2025-01-02",
      "date": "2025-01-02",
      "size": "2.5GB",
      "status": "success",
      "location": ["local", "s3", "gcs"]
    },
    {
      "id": "backup-2025-01-01",
      "date": "2025-01-01",
      "size": "2.4GB",
      "status": "success",
      "location": ["local", "s3"]
    }
  ]
}
```

### ForÃ§ar Backup Agora
```bash
POST /api/admin/backups/trigger
Authorization: Bearer {token}

Resposta:
{
  "success": true,
  "backupId": "backup-2025-01-02",
  "timestamp": "2025-01-02T10:30:00Z"
}
```

### Restaurar Backup
```bash
POST /api/admin/backups/backup-2025-01-01/restore
Authorization: Bearer {token}

Resposta:
{
  "success": true,
  "message": "RestauraÃ§Ã£o iniciada",
  "estimatedTime": "15 minutos"
}
```

---

## ğŸ§ª TESTES DE BACKUP

```typescript
describe('Backup e RestauraÃ§Ã£o', () => {
  test('Backup completo criado com sucesso', async () => {
    const result = await performBackup();
    expect(result.success).toBe(true);
    expect(result.timestamp).toBeDefined();
  });
  
  test('Arquivo de backup Ã© vÃ¡lido', async () => {
    const backupFile = await getLatestBackup();
    const isValid = await validateBackupFile(backupFile);
    expect(isValid).toBe(true);
  });
  
  test('RestauraÃ§Ã£o funciona corretamente', async () => {
    const backupFile = await getLatestBackup();
    const result = await restoreBackup(backupFile);
    expect(result.success).toBe(true);
  });
  
  test('Dados sÃ£o recuperados corretamente', async () => {
    const originalCount = await prisma.student.count();
    const backupFile = await getLatestBackup();
    await restoreBackup(backupFile);
    const restoredCount = await prisma.student.count();
    expect(restoredCount).toBe(originalCount);
  });
});
```

---

## ğŸ“‹ CHECKLIST DE IMPLEMENTAÃ‡ÃƒO

### Semana 3
- [ ] Instalar dependÃªncias (node-cron, aws-sdk, google-cloud-storage)
- [ ] Criar configuraÃ§Ã£o de backup
- [ ] Implementar script de backup
- [ ] Implementar script de restauraÃ§Ã£o
- [ ] Configurar agendamento com cron
- [ ] Testes de backup

### Semana 4
- [ ] Implementar endpoints de backup
- [ ] IntegraÃ§Ã£o com S3
- [ ] IntegraÃ§Ã£o com GCS
- [ ] Implementar notificaÃ§Ãµes
- [ ] Testes de restauraÃ§Ã£o
- [ ] DocumentaÃ§Ã£o

---

## ğŸ“Š MÃ‰TRICAS DE SUCESSO

- [ ] Backup diÃ¡rio executado com 100% de sucesso
- [ ] Tempo de backup < 30 minutos
- [ ] Tempo de restauraÃ§Ã£o < 1 hora
- [ ] RedundÃ¢ncia em 3 locais (local + S3 + GCS)
- [ ] VerificaÃ§Ã£o automÃ¡tica semanal
- [ ] Alertas de falha funcionando

---

## ğŸ’° ESTIMATIVA DE CUSTOS

| ServiÃ§o | Custo/mÃªs | Notas |
|---------|-----------|-------|
| AWS S3 | R$ 50 | 200GB armazenado |
| GCS | R$ 40 | 500GB armazenado |
| Armazenamento local | R$ 0 | Servidor existente |
| **Total** | **R$ 90** | Muito acessÃ­vel |

---

## ğŸš€ PRÃ“XIMAS FASES

ApÃ³s Backup estar implementado:
1. Fase 2: NotificaÃ§Ãµes
2. Fase 2: RelatÃ³rios
3. Fase 2: Analytics

---

**PrÃ³ximo documento:** `PHASE_2_DETAILED_PLAN.md`
